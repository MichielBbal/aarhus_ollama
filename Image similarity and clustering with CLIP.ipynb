{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a53c74b",
   "metadata": {},
   "source": [
    "# Use CLIP to find and cluster images.\n",
    "\n",
    "**International Business Days Aarhus - November 13th 2024 - Lecturer: Michiel Bontenbal, Amsterdam**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6c51e",
   "metadata": {},
   "source": [
    "In this notebook you learn about two applications for cosine similarity: finding similar images and clustering similar images.\n",
    "\n",
    "First, we'll get an understanding how the maths for cosine similarity work using a basic example.\n",
    "Next, we'll calculatie the similarity between two images.\n",
    "Third, we'll find similar images from a set of images. \n",
    "And finally, we'll cluster images based on their similarity.\n",
    "\n",
    "### Contents\n",
    "1. Intro:  calculate the cosine similarity (nl: cosinusgelijkenis)\n",
    "2. Calculate cosine similarity for 2 images\n",
    "3. Find similar images with cosine similarity\n",
    "4. Clustering images with cosine similarity and KMeans clustering\n",
    "\n",
    "### Sources\n",
    "- https://nl.wikipedia.org/wiki/Cosinusgelijkenis\n",
    "- https://openai.com/research/clip\n",
    "- https://medium.com/@jeremy-k/unlocking-openai-clip-part-2-image-similarity-bf0224ab5bb0\n",
    "- https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "- https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0cdc3",
   "metadata": {},
   "source": [
    "## 1. Intro:  calculate the cosine similarity\n",
    "\n",
    "To calculate the cosine similarity we take to vectors (A,B). \n",
    "\n",
    "We then calculate their dotproduct and devide it by the product of the normalized vectors. \n",
    "\n",
    "cos(θ) = (A · B) / (||A|| ||B||)\n",
    "\n",
    "See below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28728357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two points\n",
    "\n",
    "# Choose values [1-10] (integers/floats) to properly plot it below\n",
    "\n",
    "# point 1\n",
    "p1x = 4\n",
    "p1y = 3\n",
    "\n",
    "#point 2\n",
    "p2x = 3\n",
    "p2y = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e285af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to calculate cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum(x * y for x, y in zip(a, b))\n",
    "    magnitude_a = sum(x * x for x in a) ** 0.5\n",
    "    magnitude_b = sum(x * x for x in b) ** 0.5\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "cosine_similarity((p1x,p1y), (p2x,p2y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a4c942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGiCAYAAADNzj2mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAy2ElEQVR4nO3deViU9f7/8ReLICZi4lE0cc2y1Mq0LFsUFEqN3FpcMrVTHgtLwggxyQ0hVDTLtIOeTDM1O25tlqi4lR5xr2zTTCw17EiAGCBw//7w6/yaM2guM9w3zPNxXfzB5zPM/b7C65pnN/fc42EYhiEAAAAL8TR7AAAAgP9FoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAy7nkQNm0aZMiIiJUv359eXh4aOXKlXb7hmFo3Lhxql+/vvz8/NSpUyd9/fXXzpoXAAC4gUsOlPz8fN18882aOXNmmfuTJ0/WtGnTNHPmTGVkZCgoKEhhYWHKy8u74mEBAIB78LiSDwv08PDQihUr1LNnT0lnz57Ur19fUVFRio2NlSQVFhaqbt26Sk5O1j/+8Q+nDA0AACo3b2c+2aFDh3T8+HGFh4fb1nx9fdWxY0d98cUXZQZKYWGhCgsLbd+Xlpbq5MmTCgwMlIeHhzPHAwAALmIYhvLy8lS/fn15el75Ja5ODZTjx49LkurWrWu3XrduXR0+fLjMn0lKStL48eOdOQYAADDJkSNH1KBBgyt+HqcGyjn/e+bDMIzzng2Ji4tTdHS07fucnBw1bNhQR44cUY0aNVwxHgAAcLLc3FwFBwfL39/fKc/n1EAJCgqSdPZMSr169WzrWVlZDmdVzvH19ZWvr6/Deo0aNQgUAAAqGGddnuHU+6A0adJEQUFBSktLs60VFRVp48aN6tChgzMPBQAAKrFLPoNy6tQpHThwwPb9oUOHtGfPHtWqVUsNGzZUVFSUEhMT1bx5czVv3lyJiYmqVq2a+vfv79TBAQBA5XXJgbJjxw6FhITYvj93/cigQYP09ttv68UXX9Qff/yhZ555RtnZ2Wrfvr3WrFnjtL9JAQCAyu+K7oPiCrm5uQoICFBOTg7XoAAAUEE4+/Wbz+IBAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWI7TA6W4uFhjxoxRkyZN5Ofnp6ZNm2rChAkqLS119qEAAEAl5e3sJ0xOTtabb76p+fPnq2XLltqxY4eGDBmigIAAjRgxwtmHAwAAlZDTA2Xr1q3q0aOHunfvLklq3LixFi9erB07djj7UAAAoJJy+p947r77bq1bt07ff/+9JGnv3r3asmWLunXrVubjCwsLlZuba/cFAADcm9PPoMTGxionJ0ctWrSQl5eXSkpKNGnSJPXr16/MxyclJWn8+PHOHgMAAFRgTj+D8t5772nhwoVatGiRdu3apfnz52vq1KmaP39+mY+Pi4tTTk6O7evIkSPOHgkAAFQwHoZhGM58wuDgYI0aNUqRkZG2tYSEBC1cuFDffvvtX/58bm6uAgIClJOToxo1ajhzNAAA4CLOfv12+hmU06dPy9PT/mm9vLx4mzEAALhoTr8GJSIiQpMmTVLDhg3VsmVL7d69W9OmTdMTTzzh7EMBAIBKyul/4snLy1N8fLxWrFihrKws1a9fX/369dPLL78sHx+fv/x5/sQDAEDF4+zXb6cHypUiUAAAqHgsfw0KAADAlSJQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACzHJYHyyy+/6LHHHlNgYKCqVaumW265RTt37nTFoQAAQCXk7ewnzM7O1l133aWQkBCtXr1aderU0cGDB1WzZk1nHwoAAFRSTg+U5ORkBQcHa968eba1xo0bO/swAACgEnP6n3g++OADtWvXTg8//LDq1KmjNm3aaM6cOed9fGFhoXJzc+2+AACAe3N6oPz444+aPXu2mjdvrs8++0zDhg3Tc889pwULFpT5+KSkJAUEBNi+goODnT0SAACoYDwMwzCc+YQ+Pj5q166dvvjiC9vac889p4yMDG3dutXh8YWFhSosLLR9n5ubq+DgYOXk5KhGjRrOHA0AALhIbm6uAgICnPb67fQzKPXq1dONN95ot3bDDTcoMzOzzMf7+vqqRo0adl8AAMC9OT1Q7rrrLn333Xd2a99//70aNWrk7EMBAIBKyumB8vzzz2vbtm1KTEzUgQMHtGjRIqWmpioyMtLZhwIAAJWU0wPltttu04oVK7R48WK1atVKEydO1KuvvqoBAwY4+1AAAKCScvpFslfK2RfZAAAA17P8RbIAAABXikABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHIIFAAAYDkECgAAsBwCBQAAWA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACWQ6AAAADLIVAAAIDlECgAAMByCBQAAGA5BAoAALAcAgUAAFgOgQIAACyHQAEAAJZDoAAAAMshUAAAgOUQKAAAwHK8zR4AcKWSUkPbD51UVl6B6vhX1e1NasnL08PssQAAf4FAQaX16VfHNP7D/TqWU2BbqxdQVWMjbtT9reqZOBkA4K+4/E88SUlJ8vDwUFRUlKsPBdh8+tUxPb1wl12cSNLxnAI9vXCXPv3qmEmTAQAuhksDJSMjQ6mpqbrppptceRjATkmpofEf7pdRxt65tfEf7ldJaVmPAABYgcsC5dSpUxowYIDmzJmjq6+++ryPKywsVG5urt0XcCW2HzrpcObkzwxJx3IKtP3QyfIbCgBwSVwWKJGRkerevbu6dOlywcclJSUpICDA9hUcHOyqkeAmsvLOHyeX8zgAQPlzSaAsWbJEu3btUlJS0l8+Ni4uTjk5ObavI0eOuGIkuJE6/lWd+jgAQPlz+rt4jhw5ohEjRmjNmjWqWvWvXwB8fX3l6+vr7DHgxprVMORnFOi0fOTh4djgHpKCAs6+5RgAYE1OP4Oyc+dOZWVlqW3btvL29pa3t7c2btyo1157Td7e3iopKXH2IQFJUn5+viZNmqTm1zbT4ZUpkjwkw/5C2HN3QBkbcSP3QwEAC3P6GZTOnTvryy+/tFsbMmSIWrRoodjYWHl5eTn7kHBzZ86c0Zw5czRhwgT9+uuvZxdztyrn46lq3ne0Tpw6Y3tsEPdBAYAKwemB4u/vr1atWtmtXXXVVQoMDHRYB65EaWmpli5dqjFjxujgwYMO+09H3KmE0WHcSRYAKiDuJIsKxzAMrVmzRnFxcdq9e3eZj6lVq9bZM3aeHrqzWWA5TwgAuFLlEigbNmwoj8PADWzfvl2jRo1Senr6BR83ZswY1axZs3yGAgA4HZ9mjArh22+/1UMPPaT27dv/ZZw0btxYzzzzTDlNBgBwBf7EA8tbvny5BgwYoIKCi7uxWkJCAm9dB4AKjjMosLzevXvrl19+0b/+9S/dd999F3xsmzZt1K9fv3KaDADgKgQKKoRatWppyJAhat++/QUfl5ycLE9P/lkDQEXHn3hQIRiGobFjx2rixInnfUxYWJjCwsLKcSoAgKvwv5qwPMMwFB8f7xAnjRs3tvs+OTm5HKcCALgSgQJLMwxDL730kiZNmmS3Hh4erl27dql69eqSpAEDBqhNmzZmjAgAcAECBZZlGIZGjx7t8KnY999/v1atWqWrr75aDzzwgHx8fJSQkGDSlAAAV+AaFFiSYRgaNWqUJk+ebLfetWtXLV++3PZJ2Q899JDq1avn8OceAEDFRqDAcgzD0IsvvqipU6farXfr1k3Lli2zxYl0Nlg6duxY3iMCAFyMQIGlGIahmJgYpaSk2K13795dy5Ytc7gBW7Vq1VStWrXyHBEAUA64BgWWYRiGRo4c6RAnERERZcYJAKDy4gwKLMEwDD3//POaMWOG3fqDDz6opUuXEicA4GYIFJjOMAxFRUXptddes1vv0aOHli5dKh8fH5MmAwCYhT/xwFSGYei5555ziJNevXoRJwDgxjiDAtMYhqFnn31Wb7zxht167969tWTJElWpUsWkyQAAZuMMCkxRWlqqyMhIhzjp06cPcQIAIFBQ/s7FyezZs+3WH3roIS1evJg4AQAQKChfpaWlevrpp/Xmm2/arT/yyCNatGgRcQIAkMQ1KChHpaWlGjZsmObMmWO3/uijj2rhwoXy9uafIwDgLF4RUC5KS0s1dOhQ/etf/7Jb79u3r9555x3iBABghz/xwOVKS0v11FNPOcRJ//79iRMAQJl4ZYBLlZSU6Mknn9Tbb79ttz5gwADNnz9fXl5e5gwGALA0zqDAZUpKSvT3v//dIU4GDhxInAAALogzKHCJkpISDRkyRO+8847d+uOPP6633nqLOLlEJaWGth86qay8AtXxr6rbm9SSl6eH2WMBgMsQKHC6kpISDR48WAsXLrRbHzx4sObOnUucXKJPvzqm8R/u17GcAttavYCqGhtxo+5vVc/EyQDAdfgTD5yquLhYjz/+uEOcDBkyhDi5DJ9+dUxPL9xlFyeSdDynQE8v3KVPvzpm0mQA4FoECpzmXJwsWrTIbv3vf/87cXIZSkoNjf9wv4wy9s6tjf9wv0pKy3oEAFRsBAqcori4WAMHDtTixYvt1p988kmlpqbK05N/apdq+6GTDmdO/syQdCynQNsPnSy/oQCgnHANCq5YcXGxBgwYoKVLl9qtP/XUU3rzzTeJk8uUlXf+OLmcxwFARUKg4IqcOXNGAwYM0Pvvv2+3/o9//EOzZs0iTi5RcXGxvvzyS23dulWrdx6U/hb6lz9Tx79qOUwGAOWLQMFlO3PmjPr3769///vfdutPP/20Zs6cSZxchKysLG3btk1bt27V1q1blZGRodOnT5/d9PDUNcNay9u/tuTh+JZiD0lBAWffcgwAlQ2Bgsty5swZ9e3bV8uXL7dbf+aZZzRz5kx5lPGC6u7OnDmjffv22QXJjz/+eP4fMEpV89Ba5d/c7+y3f9o69193bMSN3A8FQKVEoOCSFRUVqW/fvlqxYoXd+vDhw/Xaa68RJ39iGIamT5+uVatWKSMjQ3/88cdF/2zr1q2VvnCGMo4VOdwHJYj7oACo5AgUXJKioiI9+uijWrlypd36s88+qxkzZhAn/8PDw0OPPfaYNm/efElx0qJFC6WlpSkwMFD3B0phNwZxJ1kAbsXDMAxL3UQhNzdXAQEBysnJUY0aNcweB39SVFSkhx9+WB988IHd+ogRIzR9+nTi5AIMw9DChQs1fPhw5ebmXvCxzZo106ZNm1S/fv1ymg4ArpyzX7+5ihEXpbCwUA899JBDnERFRREnF+HEiRPaunWr8vPzL/i4Ro0aaf369cQJALfHn3jwl87FyUcffWS3Hh0dralTpxInF1BQUKBXX31ViYmJysvLu+Bjr7nmGq1fv14NGzYsp+kAwLoIFFxQQUGB+vTpo08++cRufeTIkZoyZQpxch6lpaVasmSJ4uLilJmZ+ZePr1u3rtavX6+mTZuWw3QAYH38iQfnVVBQoN69ezvESUxMDHFyAVu2bNEdd9yhAQMGOMRJlSpVFBUVpWbNmtnWateurXXr1um6664r71EBwLIIFJSpoKBAvXr10urVq+3WY2NjlZycTJyU4cCBA+rTp4/uueceZWRkOOz37t1b+/fv1/Tp01WnTh1J0tVXX621a9eqZcuW5T0uAFgaf+KBgz/++EM9e/bUmjVr7NZHjRqlxMRE4uR/nDx5UhMnTtQbb7yhM2fOOOzfdtttSklJ0T333GNbq169umrUqKHPPvtMN998c3mOCwAVAoECO3/88Yd69OihtLQ0u/XRo0crISGBOPmToqIivfHGG5o4caKys7Md9oODg5WUlKR+/fo53PY/KChIq1ev1m233VZe4wJAhUKgwOb06dPq0aOH1q5da7c+ZswYTZgwgTj5P4ZhaPny5YqNjdXBgwcd9v39/RUXF6eoqCj5+fmV+Ryvvfaaatas6eJJAaDiIlAg6WycPPjgg1q3bp3denx8vMaPH0+c/J+MjAxFR0dry5YtDnuenp4aOnSoxo8fb7vG5HyIEwC4MAIFOn36tCIiIrR+/Xq79bFjx2rcuHHmDGUxhw8f1ujRo7Vo0aIy97t166YpU6boxhtvLOfJAKByIlDcXH5+viIiIpSenm63Pm7cOI0dO9akqawjNzdXSUlJmj59ugoLCx32b7rpJqWkpKhLly4mTAcAlReB4sby8/PVvXt3bdy40W59/Pjxevnll02ayhqKi4s1Z84cjR07VidOnHDYr1evnhISEjRo0CB5eXmZMCEAVG4Eips6deqUunfvrk2bNtmtT5w4UWPGjDFpKvMZhqFPPvlEMTEx+uabbxz2/fz8FBMTo5iYGFWvXt2ECQHAPRAobujUqVPq1q2bNm/ebLc+adIkjR492qSpzLd3716NHDnS4UJhSfLw8NCgQYOUkJCga665xoTpAMC9EChuJi8vT926dXN4F0piYqLi4uJMmspcR48eVXx8vObNmyfDMBz2Q0NDlZKSoltuuaX8hwMAN0WguJG8vDx17dpVn3/+ud36K6+8otjYWJOmMk9+fr6mTp2qyZMn6/Tp0w77LVq00JQpU9S9e3feZg0A5YxAcRO5ubnq2rWrvvjiC7v1yZMnKyYmxqSpzFFSUqIFCxZozJgxOnr0qMN+7dq1NX78eD311FOqUqWKCRMCAAgUN5CTk6P7779f27Zts1ufMmWKXnjhBZOmMsfatWv1wgsvaO/evQ57vr6+ioqKUlxcnAICAkyYDgBwDoFSyeXk5Oi+++7Tf/7zH7v1lJQURUdHmzRV+fvmm28UExOjjz/+uMz9fv36KTExUY0bNy7fwQAAZSJQKrHff/9d9913n7Zv3263Pm3aND3//PMmTVW+srKyNG7cOKWmpqqkpMRh/6677lJKSorat29vwnQAgPMhUCqp33//XeHh4crIyLBbf/XVVzVixAiTpio/BQUFevXVV5WYmKi8vDyH/WbNmik5OVm9e/fmAlgAsCACpRLKzs5WeHi4duzYYbc+Y8YMPffccyZNVT5KS0u1ZMkSxcXFKTMz02G/Zs2aevnll/XMM8/I19fXhAkBABeDQKlksrOzFRYWpp07d9qtv/766xo+fLhJU5WPLVu2KDo62uGskSR5e3srMjJS8fHxCgwMNGE6AMClIFAqkZMnTyosLEy7du2yW585c6YiIyNNmsr1Dhw4oNjYWC1fvrzM/V69eik5OVnNmzcv58kAAJeLQKkkTp48qS5dumj37t1267NmzdLTTz9t0lSudfLkSSUkJGjmzJk6c+aMw367du2UkpKie++914TpAABXgkCpBP773/+qS5cu2rNnj9367NmzNWzYMHOGcqGioiLNmjVLEyZMUHZ2tsN+cHCwkpKS1K9fP3l6epowIQDgShEoFdxvv/2mLl26ONx47J///KeGDh1q0lSuYRiGVqxYodjYWB04cMBh39/fX3FxcYqKipKfn58JEwIAnMXp/3uZlJSk2267Tf7+/qpTp4569uyp7777ztmHgc7GSefOnR3iJDU1tdLFSUZGhjp27Kg+ffo4xImnp6eGDRumH374QXFxccQJAFQCTg+UjRs3KjIyUtu2bVNaWpqKi4sVHh6u/Px8Zx/KrZ04cUKhoaHat2+fbc3Dw0Nz587VU089ZeJkzpWZmakBAwbo9ttv1+bNmx32u3btqn379mn27NmqW7euCRMCAFzBwyjr8+Wd6MSJE6pTp442btxY5sWKhYWFKiwstH2fm5ur4OBg5eTkqEaNGq4crcLKyspS586d9dVXX9nWzsXJE088YeJkzpObm6ukpCRNnz7d7t/HOa1bt1ZKSorCwsJMmA4A8L9yc3MVEBDgtNdvl19BmJOTI0mqVatWmftJSUkKCAiwfQUHB7t6pAotKytLoaGhDnHy1ltvVYo4KS4u1uzZs3XttdfqlVdecYiToKAgzZ07V7t37yZOAKASc+kZFMMw1KNHD2VnZ5d5el7iDMql+PXXXxUaGqr9+/fb1jw8PDRv3jwNGjTIxMmunGEY+uSTTxQTE6NvvvnGYd/Pz08xMTGKiYlR9erVTZgQAHAhzj6D4tJ38QwfPlz79u3Tli1bzvsYX19fbjl+EY4fP67Q0FC7F28PDw/Nnz9fAwcONHGyK7d3716NHDlS69atc9jz8PDQoEGDlJCQoGuuucaE6QAAZnBZoDz77LP64IMPtGnTJjVo0MBVh3ELx44dU2hoqL799lvbmqenp+bPn6/HHnvMxMmuzNGjRxUfH6958+aprBN5oaGhSklJ0S233FL+wwEATOX0QDEMQ88++6xWrFihDRs2qEmTJs4+hFs5duyYQkJC7N6q7enpqQULFmjAgAEmTnb58vPzNXXqVE2ePFmnT5922G/RooWmTJmi7t2780nDAOCmnB4okZGRWrRokVatWiV/f38dP35ckhQQEMD9KS7R0aNHFRISou+//9625unpqYULF6pfv34mTnZ5SkpKtGDBAo0ZM0ZHjx512K9du7bGjx+vp556SlWqVDFhQgCAVTj9Itnz/R/vvHnzNHjw4L/8eWdfZFNR/fLLLwoJCdEPP/xgW/P09NS7776rvn37mjjZ5Vm3bp1GjhzpcFM56ex1SFFRUYqLi1NAQIAJ0wEArpTlL5J18W1V3MLPP/+skJAQuzumenl56d1339Wjjz5q4mSX7ptvvlFMTIw+/vjjMvf79u2rpKQkNW7cuHwHAwBYGp/FYzE///yzOnXqpIMHD9rWvLy8tHjxYj388MMmTnZpsrKyNG7cOKWmpqqkpMRhv0OHDpo2bZrat29vwnQAAKsjUCzkyJEjCgkJcYiTJUuW6KGHHjJxsotXUFCgGTNmaNKkScrLy3PYb9q0qZKTk9WnTx8ugAUAnBeBYhGZmZkKCQnRjz/+aFvz9vbWkiVL1KdPHxMnuziGYWjJkiWKi4vT4cOHHfZr1qyp+Ph4RUZGct8bAMBfIlAs4PDhwwoJCdGhQ4dsa97e3lq6dKl69epl4mQX5/PPP1d0dLS2b9/usOft7a3IyEjFx8crMDDQhOkAABURgWKyn376SSEhIfrpp59sa97e3nr//ffVs2dP0+a6GAcPHlRsbKyWLVtW5n6vXr2UnJys5s2bl/NkAICKjkAx0U8//aROnTrZ/UmkSpUqev/999WjRw8TJ7uw7OxsTZw4UTNnztSZM2cc9tu2batp06aV+enVAABcDALFJIcOHVKnTp2UmZlpW6tSpYqWLVumiIgIEyc7v6KiIs2aNUsTJkxQdna2w35wcLASExPVv39/eXq6/IOyAQCVGIFigh9//FEhISF2ceLj46Nly5bpgQceMHGyshmGoRUrVig2Ntbu3izn+Pv7Ky4uTlFRUdwtGADgFARKOTt48KBCQkJ05MgR25qPj4+WL1+u7t27mzhZ2TIyMjRy5Eht3rzZYc/T01NDhw7VuHHjVLduXROmAwBUVgRKOTpw4IBCQkL0888/29Z8fX21YsUKde3a1cTJHGVmZiouLk6LFi0qc79r166aMmWKWrZsWc6TAQDcAYFSTg4cOKBOnTrpl19+sa35+vpq5cqVuv/++02czF5ubq6SkpI0ffp0FRYWOuy3bt1aKSkpCgsLM2E6AIC7IFDKwQ8//KBOnTrZfYKvr6+vVq1apfvuu8/Eyf6/4uJizZ07Vy+//LJOnDjhsB8UFKSEhAQNHjxYXl5eJkwIAHAnBIqLff/99+rUqZOOHTtmW6tatapWrVql8PBwEyc7yzAMrV69WjExMdq/f7/Dvp+fn2JiYhQTE6Pq1aubMCEAwB0RKC703XffKSQkxCFOPvzwQ3Xp0sXEyc7au3evXnjhBa1du9Zhz8PDQ4MGDVJCQoKuueYaE6YDALgzAsVFvv32W4WEhOj48eO2NT8/P3344Yfq3LmziZNJR48eVXx8vObNmyfDMBz2Q0NDNXXqVLVp08aE6QAAIFBc4ptvvlFISIh+/fVX25qfn58++ugjhYaGmjZXfn6+pk6dqsmTJ+v06dMO+9dff72mTJmiBx54gE8aBgCYikBxsv379ys0NNQhTj7++GOFhISYMlNJSYkWLFigMWPG2F2oe07t2rU1btw4DR06VFWqVDFhQgAA7BEoTvT1118rNDRUWVlZtrVq1arp448/VqdOnUyZaf369Ro5cqT27NnjsOfj46OoqCiNHj1aAQEB5T8cAADnQaA4yVdffaXQ0FC7t+heddVV+uSTT0z50Lxvv/1WMTEx+uijj8rc79u3r5KSktS4cePyHQwAgItAoDjBl19+qc6dOzvEyerVq3XPPfeU6ywnTpzQuHHj9M9//lMlJSUO+x06dNC0adPUvn37cp0LAIBLQaBcoX379qlz58767bffbGvVq1fX6tWrdffdd5fbHAUFBZoxY4YSExOVm5vrsN+0aVMlJyerT58+XAALALA8AuUK7N27V507d9Z///tf21r16tX16aef6q677iqXGQzD0JIlSxQXF6fDhw877NesWVNjxozR8OHD5evrWy4zAQBwpQiUy7Rnzx516dLFLk78/f316aefqkOHDuUyw+eff67o6Ght377dYc/b21uRkZGKj49XYGBgucwDAICzECiXYffu3erSpYtOnjxpW/P399dnn32mO++80+XHP3jwoGJjY7Vs2bIy93v16qXk5GQ1b97c5bMAAOAKBMol2rVrl7p06aLs7GzbWo0aNfTZZ5/pjjvucOmxs7OzNXHiRM2cOVNnzpxx2G/btq2mTZtmyruGAABwJgLlEuzcuVNhYWEOcbJmzRqXviumqKhIs2bN0oQJE+yOfU5wcLASExPVv39/eXp6umwOAADKC4FykXbs2KGwsDD9/vvvtrWAgACtWbNGt99+u0uOaRiGVq5cqRdffFEHDhxw2K9evbri4uL0/PPPy8/PzyUzAABgBgLlImRkZCgsLEw5OTm2tYCAAKWlpem2225zyTF37Nih6Ohobd682WHP09NTTz31lMaPH6+6deu65PgAAJiJQPkL27dvV3h4uF2c1KxZU2lpaWrXrp3Tj5eZmanRo0fr3XffLXO/a9eumjJlilq2bOn0YwMAYBUEygX85z//UXh4uN2Nz66++mqtXbtWt956q1OPlZubq1deeUXTp09XQUGBw37r1q2VkpKisLAwpx4XAAArIlDOY9u2bQoPD1deXp5tzRVxUlxcrLlz52rs2LF2HzJ4TlBQkBISEjR48GB5eXk57bgAAFgZgVKGrVu36r777rOLk1q1amnt2rVq06aNU45hGIZWr16tmJgY7d+/32Hfz89PL7zwgl588UVVr17dKccEAKCiIFD+x+eff677779fp06dsq0FBgZq3bp1uvnmm51yjH379mnkyJFau3atw56Hh4cef/xxJSQkqEGDBk45HgAAFQ2B8idbtmxR165dXRYnx44dU3x8vN566y0ZhuGwHxISopSUFKedpQEAoKIiUP7P5s2b1bVrV+Xn59vWateurXXr1ummm266oufOz89XSkqKJk+ebPf851x//fWaMmWKHnjgAT5pGAAAESiSpE2bNqlbt2528fC3v/1N69evV6tWrS77eUtKSrRgwQKNGTNGR48eddivXbu2xo0bp6FDh6pKlSqXfRwAACobtw+UjRs3qlu3bjp9+rRtzRlxsn79eo0cOVJ79uxx2PPx8VFUVJRGjx6tgICAyz4GAACVlVsHyoYNG9S9e3e7OKlTp47Wr19/2TdC+/bbbxUTE6OPPvqozP2+ffsqKSlJjRs3vqznBwDAHbjtJ8utX7/e4cxJ3bp1lZ6efllxcuLECUVGRqpVq1ZlxkmHDh20bds2LV68mDgBAOAvuOUZlHXr1ikiIkJ//PGHbe1cnNxwww2X9FwFBQWaMWOGEhMT7e44e07Tpk2VnJysPn36cAEsAAAXye0CZe3atYqIiLC7nXxQUJDS09PVokWLi34ewzC0ZMkSxcXF6fDhww77NWvW1JgxYzR8+HD5+vo6ZXYAANyFWwVKWlqaHnzwQbs4qVevntLT03X99ddf9PN8/vnnio6O1vbt2x32vL299cwzz+jll19WYGCgU+YGAMDduE2grFmzRg8++KAKCwtta/Xr11d6erquu+66i3qOgwcPatSoUfr3v/9d5n7Pnj2VnJx80c8HAADK5haB8tlnn6lHjx52cXLNNdcoPT1dzZs3/8ufz87OVkJCgl5//XWdOXPGYb9t27ZKSUlRx44dnTo3AADuqtIHyurVq9WrVy+7OGnQoIHS09N17bXXXvBni4qKNHv2bE2YMEEnT5502G/QoIGSkpLUv39/eXq67RuiAABwukodKJ988ol69eqloqIi21qDBg20YcMGNWvW7Lw/ZxiGVq5cqRdffFEHDhxw2K9evbri4uL0/PPPy8/PzyWzAwDgziptoHz88cfq3bu3XZwEBwcrPT39gnGyY8cOjRw5Ups2bXLY8/T01JNPPqkJEyaobt26LpkbAABU0kD58MMP1adPH7vrRRo2bKj09HQ1bdq0zJ/JzMzU6NGj9e6775a5f//992vKlClXdPt7AABwcSpdoHzwwQd66KGH7OKkUaNGSk9PV5MmTRwen5ubq1deeUXTp0+3e/vxOa1bt9bUqVMVHh7u0rkBAMD/V6kCZdWqVXr44Ycd4mTDhg0Ot5cvLi7W3LlzNXbsWGVlZTk8V1BQkCZOnKghQ4bIy8vL1aMDAIA/qTSBsmLFCj3yyCMqLi62rTVu3FgbNmxQo0aNbGuGYWj16tWKiYnR/v37HZ7Hz89PL7zwgl588UVVr169XGYHAAD2KkWgLF++XI8++qhdnDRp0kQbNmxQw4YNbWv79u3TyJEjtXbtWofn8PDw0OOPP66EhAQ1aNCgXOYGAABlq/CBsmzZMvXt29cuTpo2bar09HRbnBw7dkzx8fF66623ZBiGw3OEhIQoJSVFbdq0Kbe5AQDA+VXoQHn//ffVr18/lZSU2NaaNWum9PR0BQcHKz8/XykpKZo8ebLy8/Mdfv7666/XlClT9MADD/BJwwAAWEiFDZSlS5eqf//+dnFy7bXXKj09XfXr19fbb7+tl156SUePHnX42dq1a2vcuHEaOnSoqlSpUp5jAwCAi1AhA+W9997TgAED7OKkefPmSk9P13fffaeIiAjt2bPH4ed8fHwUFRWl0aNHKyAgoBwnBgAAl6LCBcrixYv12GOPqbS01LZ23XXXKTU1VcOGDdNHH31U5s89+uijSkpKKvNeKAAAwFoqVKAsWrRIAwcOtIuTZs2a6c4771Tnzp3tzqicc+edd2ratGm64447ynNUAABwBSrMR/C+++67DnHyt7/9Tb/++qvmz5/vECdNmjTR0qVL9fnnnxMnAABUMBXiDMo777yjwYMH28WJt7e3Tpw44fDYgIAAxcfHa/jw4fL19S3PMQEAgJNYPlAWLFigwYMHO9y/5M/3PZHOBsszzzyjl19+WYGBgeU5IgAAcDJLB8r8+fM1ZMiQMm+u9mc9e/ZUcnKyrrvuunKaDAAAuJJlr0FZuHDhX8bJrbfeqg0bNmjFihXECQAAlYjLAmXWrFlq0qSJqlatqrZt22rz5s2X9PORkZHnjZMGDRpowYIFysjIUMeOHZ0xLgAAsBCXBMp7772nqKgovfTSS9q9e7fuuecede3aVZmZmVf0vNWrV1dCQoK+++47DRw4UJ6elj0BBAAAroCH8VcXeFyG9u3b69Zbb9Xs2bNtazfccIN69uyppKQku8cWFhaqsLDQ9n1OTo7dJxCfM3jwYI0ePVp169Z19rgAAOAK5ebmKjg4WL///rtz7tZuOFlhYaHh5eVlLF++3G79ueeeM+69916Hx48dO9aQxBdffPHFF198VYKvgwcPOqUnnP4unt9++00lJSUOZzrq1q2r48ePOzw+Li5O0dHRtu9///13NWrUSJmZmXxejgWcK+IjR46oRo0aZo/j1vhdWAe/C+vgd2Ed5/4CUqtWLac8n8veZuzh4WH3vWEYDmuS5OvrW+YN1QICAvjHZiE1atTg92ER/C6sg9+FdfC7sA5nXR/q9KtMa9euLS8vL4ezJVlZWVw/AgAALorTA8XHx0dt27ZVWlqa3XpaWpo6dOjg7MMBAIBKyCV/4omOjtbAgQPVrl073XnnnUpNTVVmZqaGDRv2lz/r6+ursWPH8jk6FsHvwzr4XVgHvwvr4HdhHc7+XbjkbcbS2Ru1TZ48WceOHVOrVq00ffp03Xvvva44FAAAqGRcFigAAACXi1uxAgAAyyFQAACA5RAoAADAcggUAABgOZYLlFmzZqlJkyaqWrWq2rZtq82bN5s9kttJSkrSbbfdJn9/f9WpU0c9e/bUd999Z/ZY0NnfjYeHh6KioswexW398ssveuyxxxQYGKhq1arplltu0c6dO80ey+0UFxdrzJgxatKkifz8/NS0aVNNmDBBpaWlZo9W6W3atEkRERGqX7++PDw8tHLlSrt9wzA0btw41a9fX35+furUqZO+/vrrSz6OpQLlvffeU1RUlF566SXt3r1b99xzj7p27arMzEyzR3MrGzduVGRkpLZt26a0tDQVFxcrPDxc+fn5Zo/m1jIyMpSamqqbbrrJ7FHcVnZ2tu666y5VqVJFq1ev1v79+5WSkqKaNWuaPZrbSU5O1ptvvqmZM2fqm2++0eTJkzVlyhS9/vrrZo9W6eXn5+vmm2/WzJkzy9yfPHmypk2bppkzZyojI0NBQUEKCwtTXl7epR3IKR856CS33367MWzYMLu1Fi1aGKNGjTJpIhiGYWRlZRmSjI0bN5o9itvKy8szmjdvbqSlpRkdO3Y0RowYYfZIbik2Nta4++67zR4DhmF0797deOKJJ+zWevfubTz22GMmTeSeJBkrVqywfV9aWmoEBQUZr7zyim2toKDACAgIMN58881Lem7LnEEpKirSzp07FR4ebrceHh6uL774wqSpIJ39hEpJTvuESly6yMhIde/eXV26dDF7FLf2wQcfqF27dnr44YdVp04dtWnTRnPmzDF7LLd09913a926dfr+++8lSXv37tWWLVvUrVs3kydzb4cOHdLx48ftXst9fX3VsWPHS34td9mnGV+q3377TSUlJQ4fKFi3bl2HDx5E+TEMQ9HR0br77rvVqlUrs8dxS0uWLNGuXbuUkZFh9ihu78cff9Ts2bMVHR2t0aNHa/v27Xruuefk6+urxx9/3Ozx3EpsbKxycnLUokULeXl5qaSkRJMmTVK/fv3MHs2tnXu9Luu1/PDhw5f0XJYJlHM8PDzsvjcMw2EN5Wf48OHat2+ftmzZYvYobunIkSMaMWKE1qxZo6pVq5o9jtsrLS1Vu3btlJiYKElq06aNvv76a82ePZtAKWfvvfeeFi5cqEWLFqlly5bas2ePoqKiVL9+fQ0aNMjs8dyeM17LLRMotWvXlpeXl8PZkqysLIcSQ/l49tln9cEHH2jTpk1q0KCB2eO4pZ07dyorK0tt27a1rZWUlGjTpk2aOXOmCgsL5eXlZeKE7qVevXq68cYb7dZuuOEGLVu2zKSJ3FdMTIxGjRqlvn37SpJat26tw4cPKykpiUAxUVBQkKSzZ1Lq1atnW7+c13LLXIPi4+Ojtm3bKi0tzW49LS1NHTp0MGkq92QYhoYPH67ly5dr/fr1atKkidkjua3OnTvryy+/1J49e2xf7dq104ABA7Rnzx7ipJzdddddDm+5//7779WoUSOTJnJfp0+flqen/UuYl5cXbzM2WZMmTRQUFGT3Wl5UVKSNGzde8mu5Zc6gSFJ0dLQGDhyodu3a6c4771RqaqoyMzM1bNgws0dzK5GRkVq0aJFWrVolf39/21mtgIAA+fn5mTyde/H393e49ueqq65SYGAg1wSZ4Pnnn1eHDh2UmJioRx55RNu3b1dqaqpSU1PNHs3tREREaNKkSWrYsKFatmyp3bt3a9q0aXriiSfMHq3SO3XqlA4cOGD7/tChQ9qzZ49q1aqlhg0bKioqSomJiWrevLmaN2+uxMREVatWTf3797+0AznjbUbO9MYbbxiNGjUyfHx8jFtvvZW3tppAUplf8+bNM3s0GAZvMzbZhx9+aLRq1crw9fU1WrRoYaSmppo9klvKzc01RowYYTRs2NCoWrWq0bRpU+Oll14yCgsLzR6t0ktPTy/zNWLQoEGGYZx9q/HYsWONoKAgw9fX17j33nuNL7/88pKP42EYhuGMogIAAHAWy1yDAgAAcA6BAgAALIdAAQAAlkOgAAAAyyFQAACA5RAoAADAcggUAABgOQQKAACwHAIFAABYDoECAAAsh0ABAACW8/8A8yqXzSjh/IQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the points and create a vector from 0,0\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#get points and plot them\n",
    "points_x = p1x, p2x\n",
    "points_y = p1y, p2y\n",
    "plt.plot(points_x, points_y, 'o') \n",
    "\n",
    "#plot vectors \n",
    "plt.quiver([0, 0], [0, 0], [p1x, p2x], [p1y,p2y], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "#plot axes and show it\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e469b",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "1. play with the points and see what happens\n",
    "2. create two examples:\n",
    "    - Where cosine similarity is 1\n",
    "    - Where cosine similarity is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741872e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result cosine similarity is 1\n",
    "\n",
    "# point 1\n",
    "p1x = 0\n",
    "p1y = 1\n",
    "\n",
    "#point 2\n",
    "p2x = 1\n",
    "p2y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result cosine similarity is 0\n",
    "\n",
    "# point 1\n",
    "p1x = 2\n",
    "p1y = 1\n",
    "\n",
    "#point 2\n",
    "p2x = 1\n",
    "p2y = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb055cc",
   "metadata": {},
   "source": [
    "## 2. Calculate cosine similarity for 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93c5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Downloading torchvision-0.17.2-cp312-cp312-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "Successfully installed torch-2.2.2 torchvision-0.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-req-build-d37hqx4r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-req-build-d37hqx4r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (2024.6.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=e66d4d6121da48b429e794fb7149a44f5b56ebd7bb17fe3a5e3b00d6f920ce61\n",
      "  Stored in directory: /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-ephem-wheel-cache-b8wzl48f/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# first install packages! Uncomment if necessary\n",
    "%pip install torch torchvision\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fdd63",
   "metadata": {},
   "source": [
    "Here we will use two images and calculate how similar they are. \n",
    "\n",
    "To do so we'll create embeddings using CLIP. CLIP as several methods of creating embeddings, but here we'll use a standard Vision Transformer (ViT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac9110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "#check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#select embedding model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)#we use a Vision Transformer (ViT) for the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c5057",
   "metadata": {},
   "source": [
    "We will download the image dataset from Huggingface. Normally you do this with Hugginface Datasets package, but for now it's easier to clone the dataset as you can easily manually inspect and display the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d86c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sim_search_mini'...\n",
      "remote: Enumerating objects: 31, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (31/31), 5.78 KiB | 164.00 KiB/s, done.\n",
      "Filtering content: 100% (25/25), 4.20 MiB | 2.57 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/MichielBontenbal/sim_search_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bb946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus - STT + ollama + TTS.ipynb\n",
      "Bonus - ollama_RAG_exercises.ipynb\n",
      "Image similarity and clustering with CLIP.ipynb\n",
      "LICENSE\n",
      "README.md\n",
      "\u001b[34mimages\u001b[m\u001b[m/\n",
      "ollama.ipynb\n",
      "ollama_RAG_basics_from_scratch.ipynb\n",
      "ollama_vision_challenges.ipynb\n",
      "peter-pan.txt\n",
      "pulp-fiction-1994.pdf\n",
      "pulp_fiction.txt\n",
      "\u001b[34msim_search_mini\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09f74c",
   "metadata": {},
   "source": [
    "### TO DO: Manually inspect the images\n",
    "\n",
    "Open the folder 'sim_search_mini' and take a look at the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb1722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select two images\n",
    "image1 = \"./sim_search_mini/apple1.jpg\"\n",
    "image2= \"./sim_search_mini/apple2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b24c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do preprocessing and create embeddings for the images\n",
    "image1_preprocess = preprocess(Image.open(image1)).unsqueeze(0).to(device)\n",
    "image1_features = model.encode_image( image1_preprocess)\n",
    "\n",
    "image2_preprocess = preprocess(Image.open(image2)).unsqueeze(0).to(device)\n",
    "image2_features = model.encode_image( image2_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b917e36",
   "metadata": {},
   "source": [
    "Inspect the embeddings your created. \n",
    "Print it, the number of dimensions and the datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86961965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.1669e-02,  5.5074e-01,  2.2807e-03,  1.2043e-01, -3.9190e-01,\n",
      "         -2.3598e-01, -2.4950e-01,  6.6237e-01,  4.6721e-01,  2.7154e-01,\n",
      "         -2.9530e-04,  4.4353e-01, -8.2537e-01, -9.3244e-02,  8.8708e-02,\n",
      "         -6.4239e-01,  6.0964e-01,  2.2159e-01,  9.7070e-02, -3.9644e-01,\n",
      "         -4.1001e-01,  2.0472e-01, -1.3705e-01, -2.9576e-01, -3.1915e-02,\n",
      "         -1.6770e-01,  2.9365e-01,  2.1303e-02,  4.9584e-02,  3.6099e-03,\n",
      "          1.3282e-01,  2.4320e-01, -5.3990e-01, -9.5293e-02,  4.6058e-01,\n",
      "          6.4122e-03, -6.0671e-02, -3.5702e-01, -2.2194e-02,  5.3980e-01,\n",
      "         -6.0448e-01, -2.3985e-01,  6.1407e-01, -4.0860e-01,  3.4073e-02,\n",
      "         -1.6220e+00,  7.9201e-01,  7.8003e-02,  2.6667e-01,  2.3369e-01,\n",
      "          6.1359e-02, -3.2643e-01,  2.3459e-01,  5.7509e-03,  2.5672e-01,\n",
      "         -4.5069e-01,  3.9069e-01,  1.8588e-01, -2.6854e-01,  1.0758e-02,\n",
      "          9.7627e-01, -3.1517e-01, -9.2648e-02, -1.2333e-01, -3.3601e-01,\n",
      "          2.9820e-01,  2.7060e-02,  1.9580e-01, -3.7932e-01, -2.8294e-01,\n",
      "          8.2420e-03, -2.0005e-01, -1.5447e-01,  2.2993e-01, -1.8790e-01,\n",
      "         -6.8456e-01, -7.7499e-01,  3.1623e-01,  2.3311e-01, -4.6327e-01,\n",
      "         -4.8565e-01, -1.7461e-01,  1.9698e-01, -6.6542e-01, -2.0443e-01,\n",
      "          3.3931e-02,  1.1012e+00, -2.4288e-01,  6.1968e-01, -4.0820e-01,\n",
      "          3.7514e-01,  5.0923e-01, -6.8445e+00,  3.6069e-02, -1.5851e-01,\n",
      "         -1.9585e-01, -1.7748e-01,  1.6075e-01,  1.1964e-01,  7.8762e-01,\n",
      "          4.0023e-01, -1.1741e-01,  4.6102e-01,  4.1168e-02, -2.2215e-01,\n",
      "          1.2146e-01, -1.3052e+00,  4.6228e-01,  1.3687e-01, -3.4705e-01,\n",
      "          2.9883e-01,  9.2999e-02, -3.2019e-01, -5.9685e-02, -2.6581e-01,\n",
      "         -2.9277e-01,  2.4549e-01,  5.4868e-02,  2.8302e-01, -2.9666e-01,\n",
      "         -1.1254e-01,  1.7873e-01,  3.1564e-01, -1.5455e-01,  3.4546e-01,\n",
      "         -4.9861e-01,  5.2676e-02, -2.4958e-01,  2.8914e-01,  2.1755e-01,\n",
      "          5.3186e-01, -4.0113e-01, -2.2884e-01,  8.9107e-01, -2.2918e-01,\n",
      "         -4.0466e-02,  2.3783e-01, -4.8803e-01, -2.6483e-01, -5.1486e-02,\n",
      "         -3.5695e-01,  1.9257e-01,  1.8916e-01,  2.2621e-01, -4.5052e-01,\n",
      "          2.7385e-01,  2.3076e-01,  2.5027e-02,  8.5128e-02, -4.0123e-01,\n",
      "         -9.4445e-02,  2.6102e-04,  2.5038e-01, -3.1076e-01,  7.3236e-02,\n",
      "         -6.0188e-01,  7.2163e-02, -1.7028e-01, -1.1895e-01,  3.2919e-01,\n",
      "         -5.0827e-01, -1.2704e-01, -1.7294e-01,  4.5969e-03, -4.3147e-01,\n",
      "         -9.3013e-02,  6.1517e-01,  6.1625e-02,  3.9396e-02,  1.1767e-01,\n",
      "         -2.0688e-02, -3.2843e-02,  1.1286e-01, -3.0778e-01,  5.0713e-02,\n",
      "         -2.4159e-01, -4.8206e-01,  4.4295e-02,  5.6058e-01, -9.5562e-03,\n",
      "          2.3352e-01, -7.2792e-01, -8.8630e-03,  2.6567e-01, -7.4295e-02,\n",
      "          1.4588e-01, -2.2395e-01, -2.1718e-01, -2.9277e-01,  2.5302e-01,\n",
      "         -5.8209e-02,  4.8395e-02,  3.2872e-01,  2.0814e-01,  8.7882e-01,\n",
      "         -3.0854e-01, -2.3721e-01, -1.6099e-01,  5.1273e-03,  6.1354e-01,\n",
      "         -1.5779e-01, -4.4911e-01, -1.2668e-01, -1.8532e-01,  7.4422e-01,\n",
      "          9.1051e-02, -2.9140e-01, -1.0189e-01,  2.8708e-01,  9.6989e-03,\n",
      "          2.2579e-01,  9.3384e-01, -3.4413e-01,  1.4549e-01,  2.5861e-01,\n",
      "         -2.7348e-01,  1.0515e-01, -3.8863e-01,  7.7178e-01,  2.5785e-01,\n",
      "          3.6207e-01, -1.5359e-01,  1.7596e-01,  7.9184e-02,  1.6260e-01,\n",
      "          1.9156e-01, -4.4094e-01, -4.0692e-01,  1.7046e-01,  1.6672e-01,\n",
      "          9.2635e-03, -5.2366e-01,  2.1619e-02, -1.2186e-01,  4.8052e-01,\n",
      "          4.5086e-01, -9.5140e-02,  1.0760e-01, -8.2057e-02,  1.3176e-01,\n",
      "         -1.0043e-01,  5.5722e-01, -3.0062e-02, -3.0113e-01, -2.6755e-01,\n",
      "         -2.6712e-02,  2.1287e-01,  6.2055e-01,  1.3881e-01,  5.9369e-01,\n",
      "         -5.0052e-01,  4.2653e-01,  1.3243e-01,  1.5752e-01,  1.6746e-01,\n",
      "          3.2340e-01, -4.1451e-01,  3.4628e-01, -8.5201e-01, -1.1951e-01,\n",
      "         -5.7895e-01,  3.1441e-01, -8.8591e-02,  6.4719e-02, -2.3167e-01,\n",
      "          1.5239e-01, -4.2357e-02,  2.1097e-01,  5.4307e-01, -2.9585e-01,\n",
      "         -4.3549e-02,  2.0363e-01, -3.5142e-01, -1.0318e-01,  3.6436e-01,\n",
      "         -1.1596e-01,  2.8742e-02,  3.9805e-01,  1.1763e-01,  1.5817e-01,\n",
      "          2.1624e-01,  3.4778e-01,  2.2075e-01,  4.7919e-01,  3.6150e-01,\n",
      "          3.5383e-01, -4.0389e-01,  1.2874e-01, -2.4636e-01,  3.3664e-02,\n",
      "          5.0684e-01, -6.0235e-02,  4.6621e-01,  9.3105e-03, -3.6276e-01,\n",
      "         -1.4433e-01, -2.1476e-01,  4.8692e-01, -1.1627e-01, -2.0380e-01,\n",
      "         -7.4336e-02,  9.8359e-02,  8.8611e-02, -7.6567e-01,  3.8499e-01,\n",
      "         -2.5187e-01, -1.5811e-01,  1.2273e-01,  3.4709e-01, -1.4306e-01,\n",
      "          3.6736e-03, -4.7147e-01,  8.9162e-01, -3.8115e-01, -1.0836e-01,\n",
      "         -1.5734e-02,  4.0058e-01, -3.9862e-01,  1.2509e-01,  2.6296e-01,\n",
      "          8.5169e-02,  1.5176e+00, -3.3753e-01, -4.7581e-01, -2.6659e-01,\n",
      "          3.9208e-01,  2.6323e-01, -1.1448e-01,  7.1872e-02,  1.8259e-01,\n",
      "          5.5622e-02, -6.0260e-01,  1.3549e-01,  2.9528e-02,  8.6951e-02,\n",
      "          1.7839e-01,  2.8965e-01, -5.0829e-01,  2.9582e-01,  1.2347e-01,\n",
      "          1.1032e-01, -5.0842e-01,  1.9131e-01,  6.2478e-01,  1.2423e-01,\n",
      "          1.9333e-01,  5.9546e-02,  7.9087e-02,  3.4613e-01, -1.2947e-01,\n",
      "         -2.4439e-01, -3.6924e-01,  1.7749e-01, -2.2478e-01, -3.9017e-01,\n",
      "          3.6693e-01,  1.6672e-01, -1.8415e-01,  3.1433e-01, -1.1178e-01,\n",
      "         -3.7520e-01,  1.3430e-01,  4.3273e-01, -2.4582e-01, -4.6930e-01,\n",
      "         -1.1336e-01, -2.9887e-01,  4.7971e-01, -3.3521e-02, -5.1368e-03,\n",
      "         -7.7357e-02,  2.0507e-01, -1.0900e-02, -7.5308e-02, -8.2019e-02,\n",
      "         -9.4134e-02,  8.3401e-02,  6.2622e-02, -3.1716e-01,  2.8475e-01,\n",
      "          1.5460e-01, -4.5753e-01, -1.1969e-01,  3.4548e-02,  1.4945e-01,\n",
      "          5.2525e-01, -1.1487e-01,  3.8320e-01, -4.9946e-01, -1.1801e+00,\n",
      "          4.2414e-01,  3.4816e-01,  4.9867e-01, -1.0357e-01,  1.8671e-04,\n",
      "          4.9093e-01,  4.3502e-02, -2.6259e-01, -5.9739e-01, -4.3371e-01,\n",
      "          2.8030e-01,  6.8235e-02,  8.1906e-02, -2.5099e-01, -1.9629e-01,\n",
      "         -5.2505e-02, -1.8940e-01,  7.5423e-02,  2.1368e-01, -6.4794e-01,\n",
      "         -5.4437e-01,  2.6395e-01, -1.8973e-01,  1.8610e-01, -1.7821e-01,\n",
      "         -7.6154e-03,  3.8003e-01, -1.9486e-01,  4.2976e-01,  1.1223e-01,\n",
      "          2.8986e-01, -2.8252e-01, -6.3061e-02, -9.8457e-02, -4.3060e-03,\n",
      "          2.0911e-01, -5.6531e-01, -5.9073e-02, -1.3933e+00,  4.8106e-02,\n",
      "         -5.4697e-01,  5.1289e-01,  1.1993e+00, -1.4523e-01, -3.7305e-01,\n",
      "          3.5169e-01, -1.9350e-01, -2.3139e-01, -2.8307e-01, -1.7252e-02,\n",
      "         -1.0579e-01,  3.9387e-01, -8.7033e-02, -4.7303e-02, -9.4680e-02,\n",
      "         -1.6910e-01,  1.1665e-02,  8.4603e-02,  5.0018e-03,  1.5129e-01,\n",
      "          4.3314e-01, -2.6473e-02,  2.5739e-01, -3.4217e-01,  6.2385e-01,\n",
      "          6.9122e-01,  2.8909e-01,  3.9270e-02,  1.6633e-01, -3.2320e-01,\n",
      "         -1.8065e-01, -5.5343e-02, -5.6835e-01,  3.4001e-01,  4.1745e-01,\n",
      "          5.1292e-01,  3.4951e-02, -4.0833e-01, -3.4132e-02, -3.6742e-02,\n",
      "         -4.8534e-01, -1.3625e-01, -3.5202e-01, -2.1688e-01, -1.5753e-01,\n",
      "          7.7994e-02,  1.2014e-01, -8.2200e-02,  6.1464e-02,  1.9276e-01,\n",
      "          3.1888e-01,  1.5532e-01,  1.8474e-02,  2.0447e-01, -2.7671e-01,\n",
      "         -1.3012e-01, -2.1429e-01,  2.3844e-01,  3.8548e-01, -1.9565e-01,\n",
      "          2.2240e-01, -5.3293e-01,  2.6242e-01, -4.0434e-01, -6.2149e-03,\n",
      "         -1.2563e-01,  9.1658e-01,  2.8302e-01, -2.2662e-01,  3.2989e-01,\n",
      "         -3.7109e-02,  3.8731e-01, -6.5022e-01,  2.1353e-01,  5.6666e-01,\n",
      "         -3.9407e-01, -6.0302e-01, -1.7502e-02,  2.7876e-01, -4.6645e-02,\n",
      "         -2.3825e-02,  5.3520e-01]], grad_fn=<MmBackward0>)\n",
      "2\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "print(image1_features)\n",
    "print(image1_features.ndim)\n",
    "print(image1_features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370082b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image similarity:  0.9983330368995667\n"
     ]
    }
   ],
   "source": [
    "#calculate the cosine similarity\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "similarity = cos(image1_features[0],image2_features[0]).item()\n",
    "similarity = (similarity+1)/2\n",
    "\n",
    "print(\"Image similarity: \", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f854db",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "Calculate cosine similarity for images apple1.jpg and elephant1.jpg.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3e0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019f546",
   "metadata": {},
   "source": [
    "### Reflect on what you just did:\n",
    "- How high is the cosine similarity for apple/apple and apple/elephant?\n",
    "- Explain difference between apple/apple and apple/elephant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ed259",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4afc9e5",
   "metadata": {},
   "source": [
    "## 3. Find similar images with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50035e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/Users/michielbontenbal/Library/CloudStorage/OneDrive-HvA/GitHub/aarhus_ollama/sim_search_mini/apple2.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         image_preprocess \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(img))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m         image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image( image_preprocess)\n\u001b[1;32m     32\u001b[0m         cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCosineSimilarity(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3498\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3496\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3497\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3498\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/Users/michielbontenbal/Library/CloudStorage/OneDrive-HvA/GitHub/aarhus_ollama/sim_search_mini/apple2.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "dataset_folder = './sim_search_mini/'\n",
    "\n",
    "images = []\n",
    "for root, dirs, files in os.walk(dataset_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpg','jpeg')):\n",
    "            images.append(  root  + '/'+ file)\n",
    "\n",
    "\n",
    "#Embedding of the input image\n",
    "original_image = './sim_search_mini/face3.jpg'\n",
    "input_image = preprocess(Image.open(original_image)).unsqueeze(0).to(device) #\n",
    "input_image_features = model.encode_image(input_image)\n",
    "\n",
    "result = {}\n",
    "for img in images:\n",
    "    with torch.no_grad():\n",
    "        image_preprocess = preprocess(Image.open(img)).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image( image_preprocess)\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "        sim = cos(image_features[0],input_image_features[0]).item()\n",
    "        sim = (sim+1)/2\n",
    "        result[img]=sim\n",
    "\n",
    "\n",
    "sorted_value = sorted(result.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_res = dict(sorted_value)\n",
    "\n",
    "top_3 = dict(itertools.islice(sorted_res.items(), 3))\n",
    "\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display most similar images\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "original_image = original_image\n",
    "first = list(top_3.keys())[0]\n",
    "second = list(top_3.keys())[1]\n",
    "third = list(top_3.keys())[2]\n",
    "\n",
    "#original image\n",
    "img0 = Image(original_image, width = 400) \n",
    "\n",
    "#top 3\n",
    "img1 = Image(first, width = 400) \n",
    "img2 = Image(second, width = 400) \n",
    "img3 = Image(third, width = 400)\n",
    "\n",
    "print(\"The original image is:\")\n",
    "display(img0)\n",
    "print('The duplicate is: ')\n",
    "display(img1)\n",
    "print('And the most similar images are:')\n",
    "display(img2, img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e727b",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "Get the image that is least similar to the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the least similar image and display it\n",
    "\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa710c",
   "metadata": {},
   "source": [
    "## 4. Clustering images with cosine similarity and KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c04ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment if necessary\n",
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create the embeddings for the images\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_embeddings(image1, image2):\n",
    "    global embedding1, embedding2\n",
    "    # Preprocess and encode the first image\n",
    "    image1_preprocess = preprocess(Image.open(image1)).unsqueeze(0)\n",
    "    embedding1 = model.encode_image(image1_preprocess)\n",
    "\n",
    "    # Preprocess and encode the second image\n",
    "    image2_preprocess = preprocess(Image.open(image2)).unsqueeze(0)\n",
    "    embedding2 = model.encode_image(image2_preprocess)\n",
    "\n",
    "    return embedding1, embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to calculate cosine similarity\n",
    "import torch\n",
    "\n",
    "def calculate_image_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two sets of image features.\n",
    "    \"\"\"\n",
    "    global similarity\n",
    "    \n",
    "    # Create a cosine similarity module\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    # Calculate the cosine similarity between the first features of each image\n",
    "    similarity = cos(embedding1[0], embedding2[0]).item()\n",
    "\n",
    "    # Scale the similarity to the range [0, 1]\n",
    "    similarity = (similarity + 1) / 2\n",
    "    #print(round(similarity,8))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the images you want to use\n",
    "#images_list = ['./images/apple1.jpg', './images/apple2.jpg', './images/banana1.jpg', './images/banana2.jpg', './images/face1.jpg','./images/face2.jpg']\n",
    "\n",
    "dataset_folder = './sim_search_mini/'\n",
    "\n",
    "images_list = []\n",
    "for root, dirs, files in os.walk(dataset_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpg','jpeg')):\n",
    "            images_list.append(  root  + '/'+ file)\n",
    "images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the cosine similarity to each image in the list. This may take some time like 2 min or longer.\n",
    "cos_sim_list =[]\n",
    "for i in range(len(images_list)):\n",
    "    image1 = images_list[i]\n",
    "    for j in range(len(images_list)):\n",
    "        image2 = images_list[j]\n",
    "        get_image_embeddings(image1, image2)\n",
    "        calculate_image_similarity(embedding1, embedding2)\n",
    "        cos_sim_list.append(similarity)\n",
    "\n",
    "print(cos_sim_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert this list to a numpy array\n",
    "import numpy as np\n",
    "num_rows = len(images_list)\n",
    "cosine_similarity_matrix = np.array(cos_sim_list).reshape(num_rows, -1)\n",
    "cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3888f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 4\n",
    "For KMeans you'll have to set the number of clusters manuall. \n",
    "Do so in the code below by replacing the question mark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a157fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the clustering with the K-Means algo and show it using Matplotlib. \n",
    "# Warning: this is a little different because we do clustering based on 1 Dimension (instead of 2 or more)\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert cosine similarity to distance (1 - similarity)\n",
    "distance_matrix = 1 - cosine_similarity_matrix\n",
    "\n",
    "# Choose the number of clusters (k)\n",
    "num_clusters = ?\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(distance_matrix)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Print the cluster assignments\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Image {i} is in cluster {label}\")\n",
    "\n",
    "# Optional: Visualize the clustering result\n",
    "# Here we assume you have 2D data, for visualization purposes only\n",
    "plt.scatter(distance_matrix[:, 0], distance_matrix[:, 1], c=labels)\n",
    "plt.title('Clustering of Images based on Cosine Similarity')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7470b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a result list and merge it with the images_list to check your result manually\n",
    "result_list = []\n",
    "for i, label in enumerate(labels):\n",
    "    #print(f\"Image {i} is in cluster {label}\")\n",
    "    result_list.append(label)\n",
    "result_list\n",
    "\n",
    "# Merge the lists into a dictionary\n",
    "merged_dict = dict(zip(images_list, result_list))\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(merged_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf183e06",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We've succesfully clustered a set of diverse images. To achieve this, we have calculated the cosine similartity for each image to the others. We've used a standerd KMeans clustering algorith to cluster the images and used Matplotlib to plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eeaf3",
   "metadata": {},
   "source": [
    "### Reflection questions\n",
    "\n",
    "#### 1. Write down the steps you did to get to the clustering in your own words.\n",
    "\n",
    "\n",
    "#### 2. Describe an embedding in your own words\n",
    "\n",
    "\n",
    "\n",
    "#### 3: Hard question. How can we speed up the calculation of the cosine similarity?\n",
    "Hint: check the datatype of the embedding. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263a58a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
